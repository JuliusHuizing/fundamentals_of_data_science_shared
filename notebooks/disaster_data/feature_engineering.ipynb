{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julius/repos/fundamentals_project/.venv/lib/python3.11/site-packages/openpyxl/styles/stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Total Deaths</th>\n",
       "      <th>Year</th>\n",
       "      <th>Disaster Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cabo Verde</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>1900</td>\n",
       "      <td>Drought</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>India</td>\n",
       "      <td>1250000.0</td>\n",
       "      <td>1900</td>\n",
       "      <td>Drought</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Canada</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1903</td>\n",
       "      <td>Mass movement (dry)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Canada</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1905</td>\n",
       "      <td>Mass movement (dry)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Belgium</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1906</td>\n",
       "      <td>Flood</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Country  Total Deaths  Year        Disaster Type\n",
       "0   Cabo Verde       11000.0  1900              Drought\n",
       "1        India     1250000.0  1900              Drought\n",
       "6       Canada          76.0  1903  Mass movement (dry)\n",
       "12      Canada          18.0  1905  Mass movement (dry)\n",
       "16     Belgium           6.0  1906                Flood"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.disaster_data_utils import *\n",
    "import numpy as np\n",
    "df = build_clean_dataframe()\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each country, for each year, we want one feature vector containg good predictors for the content of the speech they give that year.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Country', 'Total Deaths', 'Year', 'Disaster Type'], dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_vector_v1(df, country, year) -> np.array:\n",
    "    '''\n",
    "    Returns a feature vector for the given country in the given year using the information present in the provided dataframe.\n",
    "\n",
    "            Parameters:\n",
    "                    df (pd.DataFrame): The dataframe containing the disaster data\n",
    "                    country (string): The country to build the feature vector for\n",
    "                    year (int): The year to build the feature vector for\n",
    "\n",
    "            Returns:\n",
    "                    vector (np.array): a feature vector for the given country in the given year\n",
    "    '''\n",
    "    row = df[(df['Country'] == country) & (df['Year'] == year)]\n",
    "    num_disasters = len(row)\n",
    "    num_deaths = row['Total Deaths'].sum()\n",
    "    num_deaths_per_disaster = num_deaths / num_disasters if num_disasters > 0 else 0\n",
    "    num_deaths_at_biggest_disaster = row['Total Deaths'].max()\n",
    "    vector = np.array([num_disasters, num_deaths, num_deaths_per_disaster, num_deaths_at_biggest_disaster])\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5. , 322. ,  64.4, 143. ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_feature_vector_v1(df, 'Indonesia', 2005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1. From what year onwards are we going to use the data?\n",
    "  - I.e. from what year onwards is the data complete / accuracte?\n",
    "  - i.e. from what year onwards is climate change a theme that governments talk about?\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X{array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "\n",
    "def create_feature_matrix(df, years, feature_vector_builder = build_feature_vector_v1) -> np.array:\n",
    "    \"\"\"\n",
    "    Create a feature matrix for a given DataFrame and list of years.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame containing data for different countries and years.\n",
    "    - years (list): A list of years for which feature vectors should be created.\n",
    "    - feature_vector_builder (callable): A function used to build feature vectors for each country and year.\n",
    "                                         Default is build_feature_vector_v1.\n",
    "\n",
    "    Returns:\n",
    "    - np.array: A 2D numpy array representing the feature matrix, where each row corresponds to a country-year pair.\n",
    "\n",
    "    Example:\n",
    "    ```\n",
    "    from utils.disaster_data_utils import *\n",
    "    import numpy as np\n",
    "    df = build_dataframe()\n",
    "    df = build_clean_dataframe(df)\n",
    "    feature_matrix = create_feature_matrix(df, np.arange(2000, 2005))\n",
    "    ```\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for country in df['Country'].unique():\n",
    "        for year in years:\n",
    "            row = df[(df['Country'] == country) & (df['Year'] == year)]\n",
    "            if len (row) != 0:\n",
    "                last_row = row\n",
    "                feature_vector = feature_vector_builder(df, country, year)\n",
    "                result.append(feature_vector)\n",
    "            \n",
    "    return np.array(result)\n",
    "\n",
    "def create_feature_matrix(df, countries, years, feature_vector_builder=build_feature_vector_v1) -> np.array:\n",
    "    \"\"\"\n",
    "    Create a feature matrix for a given DataFrame and list of years.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame containing data for different countries and years.\n",
    "    - years (list): A list of years for which feature vectors should be created.\n",
    "    - feature_vector_builder (callable): A function used to build feature vectors for each country and year.\n",
    "                                         Default is build_feature_vector_v1.\n",
    "\n",
    "    Returns:\n",
    "    - np.array: A 2D numpy array representing the feature matrix, where each row corresponds to a country-year pair.\n",
    "\n",
    "    Example:\n",
    "    ```\n",
    "    from utils.disaster_data_utils import *\n",
    "    import numpy as np\n",
    "    df = build_dataframe()\n",
    "    df = build_clean_dataframe(df)\n",
    "    feature_matrix = create_feature_matrix(df, np.arange(2000, 2005))\n",
    "    ```\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for country in countries:\n",
    "        for year in years:\n",
    "            row = df[(df['Country'] == country) & (df['Year'] == year)]\n",
    "            if len (row) != 0:\n",
    "                last_row = row\n",
    "                feature_vector = feature_vector_builder(df, country, year)\n",
    "                result.append(feature_vector)\n",
    "            \n",
    "    return np.array(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1834, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_years = np.arange(2000, 2021)\n",
    "X_train = create_feature_matrix(df, train_years)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  3.,  3.,  3.],\n",
       "       [ 1.,  9.,  9.,  9.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       ...,\n",
       "       [ 1.,  9.,  9.,  9.],\n",
       "       [ 1., 12., 12., 12.],\n",
       "       [ 1.,  4.,  4.,  4.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression(fit_intercept=True) \n",
    "X_train = X_train\n",
    "# TODO: create proper y targets based on speech data.\n",
    "# let's first try to overfit to verify we have implemented everything correctly\n",
    "## create a y vector with a one if there are more than 10 disasters, 0 otherwise\n",
    "Y_train = np.array([1 if x > 10 else 0 for x in X_train[:, 0]])\n",
    "# Y_train = np.random.rand(X_train.shape[0]).reshape(-1, 1)\n",
    "# TODO: fix Nan Values\n",
    "model.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.07970634]\n",
      "[4.03815589]\n"
     ]
    }
   ],
   "source": [
    "# create random y vector\n",
    "y1 = np.random.rand(X_train.shape[1]).reshape(1, -1)\n",
    "y1[0] = 0\n",
    "y2 = np.random.rand(X_train.shape[1]).reshape(1, -1)\n",
    "y2[0] = 100\n",
    "print(model.predict(y1))\n",
    "print(model.predict(y2)) # seems to work as expeted; giving higher output for samples with more disasters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineer training labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/julius/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3333333333333333, 0.0, 1.0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "\n",
    "# https://www.w3resource.com/python-exercises/nltk/nltk-tokenize-exercise-3.php\n",
    "# words = word_tokenize('klimaarverandering is erg')\n",
    "\n",
    "def convert_text_to_keyword_counts(speech_string, keywords):\n",
    "    keywords = [keyword.lower() for keyword in keywords]\n",
    "    result = 0\n",
    "    words = word_tokenize(speech_string)\n",
    "    words = [word.lower() for word in words]\n",
    "    for word in words:\n",
    "        if word in keywords:\n",
    "            result += 1\n",
    "    return result\n",
    "    \n",
    "    \n",
    "\n",
    "def convert_list_of_speeches_to_list_of_keyword_counts(speeches, keywords):\n",
    "    result = []\n",
    "    for speech in speeches:\n",
    "        result.append(convert_text_to_keyword_counts(speech, keywords))\n",
    "    return result\n",
    "\n",
    "\n",
    "print(convert_list_of_speeches_to_list_of_keyword_counts(['klimaatverandering is erg', \n",
    "                                               'Pilkes is geen Pickle. Toch.', \n",
    "                                               'Minder CO2! Minder CO2! Fossielle brandstoffen zijn stom.'], \n",
    "                                              keywords=['klimaatverandering', \"CO2\", \"fossielle\"]))\n",
    "\n",
    "def convert_list_of_speeches_to_normalized_scores(speeches, keywords):\n",
    "    '''\n",
    "        \n",
    "    \n",
    "    '''\n",
    "    keywords = [keyword.lower() for keyword in keywords]\n",
    "    list_of_counts = convert_list_of_speeches_to_list_of_keyword_counts(speeches, keywords)\n",
    "    # ensure all counts have a value between 0 and 1\n",
    "    max_count = max(list_of_counts)\n",
    "    min_count = min(list_of_counts)\n",
    "    denom = max_count - min_count \n",
    "    if denom == 0:\n",
    "        denom = 1\n",
    "    normalized_counts = [(x - min_count) / (denom) for x in list_of_counts]\n",
    "    return normalized_counts\n",
    "\n",
    "\n",
    "\n",
    "convert_list_of_speeches_to_normalized_scores(['klimaatverandering is erg', \n",
    "                                               'Pilkes is geen Pickle. Toch.', \n",
    "                                               'Minder CO2! Minder CO2! Fossiele brandstoffen zijn stom.'], \n",
    "                                              keywords=['klimaatverandering', \"CO2\", \"fossiele\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "df = pd.read_pickle(\"../../data/DF_UNsession_rawtxt_per_country_from1990.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_target_value(df, country, year) -> float:\n",
    "    '''\n",
    "    Returns a target value for the given country in the given year using the information present in the provided dataframe.\n",
    "\n",
    "            Parameters:\n",
    "                    df (pd.DataFrame): The dataframe containing the disaster data\n",
    "                    country (string): The country to build the feature vector for\n",
    "                    year (int): The year to build the feature vector for\n",
    "\n",
    "            Returns:\n",
    "                    target_value (float): a target value for the given country in the given year\n",
    "    '''\n",
    "    \n",
    "    keyword = \"global warming\" \n",
    "    ps = PorterStemmer()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    row = df[(df['Country'] == country) & (df['Year'] == year)]\n",
    "    num_disasters = len(row)\n",
    "    return num_disasters\n",
    "\n",
    "\n",
    "def build_tokenized_text(df, country, year, tokenizer=RegexpTokenizer(r'\\w+'), stemmer=PorterStemmer()) -> str:\n",
    "    sub_df = df[(df['Country'] == country) & (df['Year'] == year)]\n",
    "    df_of_year = df.loc[df[\"year\"]==year]\n",
    "    corpus_of_year = ' '.join(sub_df.txt)\n",
    "    corpus_tokanized = stem_tokenizer(corpus_of_year, stemmer, tokenizer)\n",
    "    \n",
    "\n",
    "def stem_tokenizer(txt, stemmer, tokenizer):\n",
    "    txt = tokenizer.tokenize(txt.lower())\n",
    "    txt = [stemmer.stem(word) for word in txt]\n",
    "    txt = ' '.join(txt)\n",
    "    return txt\n",
    "\n",
    "def stem_counter(keyword, corpus, stemmer, tokenizer):\n",
    "\n",
    "    keyword = stem_tokenizer(keyword, stemmer, tokenizer)\n",
    "\n",
    "    corpus = stem_tokenizer(corpus, stemmer, tokenizer)\n",
    "\n",
    "    return corpus.count(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fundamentals-project-Gbp0IE_5-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
