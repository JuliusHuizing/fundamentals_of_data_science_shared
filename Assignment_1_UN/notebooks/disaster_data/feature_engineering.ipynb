{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.disaster_data_utils import *\n",
    "import numpy as np\n",
    "df = build_clean_dataframe()\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each country, for each year, we want one feature vector containg good predictors for the content of the speech they give that year.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_vector_v1(df, country, year) -> np.array:\n",
    "    '''\n",
    "    Returns a feature vector for the given country in the given year using the information present in the provided dataframe.\n",
    "\n",
    "            Parameters:\n",
    "                    df (pd.DataFrame): The dataframe containing the disaster data\n",
    "                    country (string): The country to build the feature vector for\n",
    "                    year (int): The year to build the feature vector for\n",
    "\n",
    "            Returns:\n",
    "                    vector (np.array): a feature vector for the given country in the given year\n",
    "    '''\n",
    "    row = df[(df['Country'] == country) & (df['Year'] == year)]\n",
    "    num_disasters = len(row)\n",
    "    num_deaths = row['Total Deaths'].sum()\n",
    "    num_deaths_per_disaster = num_deaths / num_disasters if num_disasters > 0 else 0\n",
    "    num_deaths_at_biggest_disaster = row['Total Deaths'].max()\n",
    "    vector = np.array([num_disasters, num_deaths, num_deaths_per_disaster, num_deaths_at_biggest_disaster])\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_feature_vector_v1(df, 'Indonesia', 2005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1. From what year onwards are we going to use the data?\n",
    "  - I.e. from what year onwards is the data complete / accuracte?\n",
    "  - i.e. from what year onwards is climate change a theme that governments talk about?\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X{array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "\n",
    "\n",
    "def create_feature_train_matrix(countries, years, feature_vector_builder=build_feature_vector_v1) -> np.array:\n",
    "    \"\"\"\n",
    "    Create a feature matrix for a given DataFrame and list of years.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame containing data for different countries and years.\n",
    "    - years (list): A list of years for which feature vectors should be created.\n",
    "    - feature_vector_builder (callable): A function used to build feature vectors for each country and year.\n",
    "                                         Default is build_feature_vector_v1.\n",
    "\n",
    "    Returns:\n",
    "    - np.array: A 2D numpy array representing the feature matrix, where each row corresponds to a country-year pair.\n",
    "\n",
    "    Example:\n",
    "    ```\n",
    "    from utils.disaster_data_utils import *\n",
    "    import numpy as np\n",
    "    df = build_dataframe()\n",
    "    df = build_clean_dataframe(df)\n",
    "    feature_matrix = create_feature_matrix(df, np.arange(2000, 2005))\n",
    "    ```\n",
    "    \"\"\"\n",
    "    df = build_clean_dataframe()\n",
    "    result = []\n",
    "    for country in countries:\n",
    "        for year in years:\n",
    "            row = df[(df['Country'] == country) & (df['Year'] == year)]\n",
    "            if len (row) != 0:\n",
    "                last_row = row\n",
    "                feature_vector = feature_vector_builder(df, country, year)\n",
    "                result.append(feature_vector)\n",
    "            \n",
    "    return np.array(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_years = np.arange(2000, 2021)\n",
    "countries = df['Country'].unique()\n",
    "X_train = create_feature_train_matrix(countries, train_years)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression(fit_intercept=True) \n",
    "X_train = X_train\n",
    "# TODO: create proper y targets based on speech data.\n",
    "# let's first try to overfit to verify we have implemented everything correctly\n",
    "## create a y vector with a one if there are more than 10 disasters, 0 otherwise\n",
    "Y_train = np.array([1 if x > 10 else 0 for x in X_train[:, 0]])\n",
    "# Y_train = np.random.rand(X_train.shape[0]).reshape(-1, 1)\n",
    "# TODO: fix Nan Values\n",
    "model.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random y vector\n",
    "y1 = np.random.rand(X_train.shape[1]).reshape(1, -1)\n",
    "y1[0] = 0\n",
    "y2 = np.random.rand(X_train.shape[1]).reshape(1, -1)\n",
    "y2[0] = 100\n",
    "print(model.predict(y1))\n",
    "print(model.predict(y2)) # seems to work as expeted; giving higher output for samples with more disasters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineer training labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tokenize import word_tokenize\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "\n",
    "\n",
    "# # https://www.w3resource.com/python-exercises/nltk/nltk-tokenize-exercise-3.php\n",
    "# # words = word_tokenize('klimaarverandering is erg')\n",
    "\n",
    "# def convert_text_to_keyword_counts(speech_string, keywords):\n",
    "#     keywords = [keyword.lower() for keyword in keywords]\n",
    "#     result = 0\n",
    "#     words = word_tokenize(speech_string)\n",
    "#     words = [word.lower() for word in words]\n",
    "#     for word in words:\n",
    "#         if word in keywords:\n",
    "#             result += 1\n",
    "#     return result\n",
    "    \n",
    "    \n",
    "\n",
    "# def convert_list_of_speeches_to_list_of_keyword_counts(speeches, keywords):\n",
    "#     result = []\n",
    "#     for speech in speeches:\n",
    "#         result.append(convert_text_to_keyword_counts(speech, keywords))\n",
    "#     return result\n",
    "\n",
    "\n",
    "# print(convert_list_of_speeches_to_list_of_keyword_counts(['klimaatverandering is erg', \n",
    "#                                                'Pilkes is geen Pickle. Toch.', \n",
    "#                                                'Minder CO2! Minder CO2! Fossielle brandstoffen zijn stom.'], \n",
    "#                                               keywords=['klimaatverandering', \"CO2\", \"fossielle\"]))\n",
    "\n",
    "# def convert_list_of_speeches_to_normalized_scores(speeches, keywords):\n",
    "#     '''\n",
    "        \n",
    "    \n",
    "#     '''\n",
    "#     keywords = [keyword.lower() for keyword in keywords]\n",
    "#     list_of_counts = convert_list_of_speeches_to_list_of_keyword_counts(speeches, keywords)\n",
    "#     # ensure all counts have a value between 0 and 1\n",
    "#     max_count = max(list_of_counts)\n",
    "#     min_count = min(list_of_counts)\n",
    "#     denom = max_count - min_count \n",
    "#     if denom == 0:\n",
    "#         denom = 1\n",
    "#     normalized_counts = [(x - min_count) / (denom) for x in list_of_counts]\n",
    "#     return normalized_counts\n",
    "\n",
    "\n",
    "\n",
    "# convert_list_of_speeches_to_normalized_scores(['klimaatverandering is erg', \n",
    "#                                                'Pilkes is geen Pickle. Toch.', \n",
    "#                                                'Minder CO2! Minder CO2! Fossiele brandstoffen zijn stom.'], \n",
    "#                                               keywords=['klimaatverandering', \"CO2\", \"fossiele\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem import PorterStemmer\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# def create_target_value_matrix(countries, years, keywords, stemmer=PorterStemmer(), tokenizer=RegexpTokenizer(r'\\w+')) -> np.array:\n",
    "#     df = create_prepoccesed_df(countries=countries, years=years, keywords=keywords, stemmer=stemmer, tokenizer=tokenizer)\n",
    "#     result = []\n",
    "#     for country in countries:\n",
    "#         for year in years:\n",
    "#             row = df[(df['country'] == country) & (df['year'] == year)]\n",
    "#             if len(row) != 1:\n",
    "#                 print(\"this should not happen\")\n",
    "#             result.append(row[\"year_score\"])\n",
    "#     return np.array(result)\n",
    "\n",
    "\n",
    "# def create_prepoccesed_df(countries, years, keywords, stemmer=PorterStemmer(), tokenizer=RegexpTokenizer(r'\\w+')):\n",
    "#     print(\"prepoccesing df\")\n",
    "#     df = pd.read_pickle(\"../../data/DF_UNsession_rawtxt_per_country_from1990.pkl\")\n",
    "#     ## filter df for countries and years\n",
    "#     df = df[df[\"country\"].isin(countries)]\n",
    "#     df = df[df[\"year\"].isin(years)]\n",
    "#     df = create_stemmed_df(df, stemmer, tokenizer)\n",
    "#     # df = add_stemmed_df_year(df, stemmer, tokenizer)\n",
    "#     df = add_keyword_counts_column(df, keywords, stemmer, tokenizer)\n",
    "#     df = add_year_score_column(df, stemmer, tokenizer)\n",
    "#     return df\n",
    "         \n",
    "# def create_stemmed_df(df, stemmer=PorterStemmer(), tokenizer=RegexpTokenizer(r'\\w+')):\n",
    "#     df_tokenized_per_country = pd.DataFrame()\n",
    "#     for row in df.iterrows():\n",
    "#         country = row[1][\"country\"]\n",
    "#         year = row[1][\"year\"]\n",
    "#         txt = row[1][\"txt\"]\n",
    "\n",
    "#         txt_tokenized = stem_tokenizer(txt, stemmer, tokenizer)\n",
    "        \n",
    "#         # Create a new row DataFrame with year and country and the tokenized text\n",
    "#         new_row = pd.DataFrame({\"country\":[country], \"year\": [year], \"txt_stemmed\": [txt_tokenized]})\n",
    "        \n",
    "#         # Concatenate the new row DataFrame to df_tokenized_per_year\n",
    "#         df_tokenized_per_country = pd.concat([df_tokenized_per_country, new_row], ignore_index=True)\n",
    "#     return df_tokenized_per_country         \n",
    "         \n",
    "# def add_year_score_column(df, stemmer=PorterStemmer(), tokenizer=RegexpTokenizer(r'\\w+')):\n",
    "#     print(\"adding year score column to df\")\n",
    "#     for year in df[\"year\"].unique():\n",
    "#         df_year = df.loc[df[\"year\"]==year]\n",
    "#         max_count_year = df_year[\"keyword_counts\"].max()\n",
    "#         min_count_year = df_year[\"keyword_counts\"].min()\n",
    "#         # for each country, add normalized score columm to df for that year\n",
    "#         df.loc[df[\"year\"]==year, \"year_score\"] = df_year[\"keyword_counts\"].apply(lambda x: normalize_score_min_max(x, min_count_year, max_count_year))\n",
    "#     return df\n",
    "              \n",
    "\n",
    "# def add_keyword_counts_column(df, keywords, stemmer=PorterStemmer(), tokenizer=RegexpTokenizer(r'\\w+')):\n",
    "#     '''\n",
    "#     Returns a dataframe with a column containing the stemmed text of the original dataframe.\n",
    "\n",
    "#             Parameters:\n",
    "#                     df (pd.DataFrame): The dataframe containing the disaster data\n",
    "#                     stemmer (PorterStemmer): The stemmer to use\n",
    "#                     tokenizer (RegexpTokenizer): The tokenizer to use\n",
    "\n",
    "#             Returns:\n",
    "#                     df (pd.DataFrame): The dataframe containing the disaster data with an extra column containing the stemmed text\n",
    "#     '''\n",
    "#     keywords = [stem_tokenizer(keyword, stemmer=stemmer, tokenizer=tokenizer) for keyword in keywords]\n",
    "\n",
    "#     print(\"adding keyword counts column to df by matching keywords in stemmed stemmed text\")\n",
    "#     df['keyword_counts'] = df['txt_stemmed'].apply(lambda x: count_keywords_in_text(keywords, x))\n",
    "#     return df\n",
    "\n",
    "\n",
    "# def build_tokenized_text(df, country, year, tokenizer=RegexpTokenizer(r'\\w+'), stemmer=PorterStemmer()) -> str:\n",
    "#     sub_df = df[(df['Country'] == country) & (df['Year'] == year)]\n",
    "#     df_of_year = df.loc[df[\"year\"]==year]\n",
    "#     corpus_of_year = ' '.join(sub_df.txt)\n",
    "#     corpus_tokanized = stem_tokenizer(corpus_of_year, stemmer, tokenizer)\n",
    "    \n",
    "\n",
    "# def stem_tokenizer(txt, stemmer, tokenizer):\n",
    "#     txt = tokenizer.tokenize(txt.lower())\n",
    "#     txt = [stemmer.stem(word) for word in txt]\n",
    "#     txt = ' '.join(txt)\n",
    "#     return txt\n",
    "\n",
    "# def count_keywords_in_text(keywords, text):\n",
    "#     count = 0\n",
    "#     for keyword in keywords:\n",
    "#         count  += text.count(keyword)\n",
    "#     return count\n",
    "\n",
    "# # def stem_counter_multiple_keywords(keywords, corpus, stemmer, tokenizer):\n",
    "# #     count = 0\n",
    "# #     for keyword in keywords:\n",
    "# #         count  += stem_counter(keyword, corpus, stemmer, tokenizer)\n",
    "# #     return count\n",
    "        \n",
    "\n",
    "# # def stem_counter(keyword, corpus, stemmer, tokenizer):\n",
    "\n",
    "# #     keyword = stem_tokenizer(keyword, stemmer, tokenizer)\n",
    "\n",
    "# #     corpus = stem_tokenizer(corpus, stemmer, tokenizer)\n",
    "\n",
    "# #     return corpus.count(keyword)\n",
    "\n",
    "\n",
    "# def normalize_score_min_max(score, min_count, max_count):\n",
    "#     if max_count == min_count:\n",
    "#         denom = 0\n",
    "#     else :\n",
    "#         denom = max_count - min_count\n",
    "#     return (score - min_count) / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../../data/DF_UNsession_rawtxt_per_country_from1990.pkl\")\n",
    "countries = df['country'].unique()\n",
    "years = df['year'].unique()\n",
    "print(countries, years)\n",
    "\n",
    "# test = create_prepoccesed_df(countries, [2009, 2010], keywords=[\"climate\", \"CO2\"])\n",
    "# test = create_target_value_matrix(countries[:3], years[20009, 2010], [\"climate\", \"CO2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = create_target_value_matrix(countries, [2009, 2010], [\"climate\", \"CO2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_and_target_value_matrices(countries, years, keywords, stemmer=PorterStemmer(), tokenizer=RegexpTokenizer(r'\\w+')) -> np.array:\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fundamentals-project-Gbp0IE_5-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
